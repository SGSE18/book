# Digitalisierung
Autor: Björn E.-M.

## Einleitung und Definition

„Digitalisierung“ - Das aktuelle Hype-Thema in der Politik und Wirtschaft. Sie reicht dabei von digitalen Sprachassistenten für Zuhause über Wearables und Satellitentechnik zu vollautomatisieren Fertigungsstraßen in der Industrie. In so gut wie allen Bereichen des Lebens lässt sie sich finden. Inzwischen existieren vollständige digitale Währungen wie z.B. „Bitcoin“ und man kann während einem Ausflug per Smartphone-App und einem internetfähigen Futterspender die Katze in der Heimat füttern und sie dabei sogar noch über eine Kamera im Livestream beobachten. 

Häufig unter anderen Begrifflichkeiten wie „Big-Data“, „Internet of Things“ und „Industrie 4.0“ die als Synonyme verwendet werden. Oftmals wird der Begriff und die Synonyme auch noch falsch verwendet und somit entsteht eine Verwässerung des Begriffs und eine falsche Zielsetzung bei der Umsetzung.

Da der Begriff hat sich in den letzten Jahren gewandelt und steht für mehr als die ursprüngliche Bedeutung. Der Begriff kann aus vielen Blickwinkeln gesehen werden. Den volkswirtschaftlichen Blickwinkel wählt z.B. die Regierung. Das Bundesministerium für Wirtschaft (BMWi) hat den Begriff und die damit verbundene Zielsetzung wie folgt definiert:

„Die Digitalisierung ist in vollem Gange. Sie betrifft uns alle – und sorgt für einen tiefgreifenden Wandel in jedem Lebensbereich. Die digitale Transformation eröffnet dabei große Chancen für mehr Lebensqualität, revolutionäre Geschäftsmodelle und effizienteres Wirtschaften. Das BMWi gestaltet diesen Wandel durch kluge Rahmenbedingungen und gezielte Förderung mit, um die digitalen Potentiale zu nutzen und neue Herausforderungen erfolgreich zu meistern.“

Bei dieser Betrachtungsweise sollen die Chancen, die durch die Digitalisierung eröffnet werden in allen Wirtschaftsbereichen und in der Gesellschaft genutzt werden. Dafür werden Förderprogramme ins Leben gerufen und der Ausbau der digitalen Infrastruktur vorangetrieben. Auf Themen wie digitale Wirtschaft und digitales Arbeiten, digitale Gesundheit, Bildung, Forschung und Sicherheit wird dabei der Fokus gelegt. 

In der Wirtschaft selbst wird versucht die Digitalisierung in betriebswirtschaftlichen Berechnungen festzuhalten, da die Erfolge im Vordergrund stehen sollen. Wodurch in Unternehmen häufig andere Ziele als die des BMWi verfolgt werden. Die Ziele sollten jedoch gleichgerichtet sein.

Die ursprüngliche Bedeutung des Begriffs „Digitalisierung“ stammt aus der Informationstechnologie (IT) und steht für die Umwandlung von analogen Werten in digitale Werte und Formate. Bei dieser Betrachtung wird ein technologischer Blickwinkel eingesetzt um den Themenkomplex der Digitalisierung zu betrachten. Im technologischen Blickwinkel stehen Begriffe wie Netzwerke, Bandbreiten, Prozessortakt, Bus-Takt und der neuste Trend die „artificial intelligence“ (AI) bzw. „Künstliche Intelligenz“ (KI) im Fokus. 

Die bisher gezeigten Beispiele sollten darstellen wie breitgefächert der Begriff der Digitalisierung ist. Eine einheitliche anerkannte Definition zu erstellen ist daher schwierig. Die freie Enzyklopädie „Wikipedia“ z.B. führt die ursprüngliche Definition des Begriffs mit auf und definiert Digitalisierung wie folgt: „Digitalisierung steht für die Überführung analoger Größen in diskrete (abgestufte) Werte.“ Das Wirtschaftslexikon „Gabler“ definiert Digitalisierung hingegen wie folgt: „Der Begriff Digitalisierung kann die digitale Revolution, die auch als dritte Revolution bekannt ist, bzw. die digitale Wende meinen.“

Häufig fallen in Verbunding mit solchen Definitionen zusätzliche Begriffe wie papierlose Organisation, Big Data, Analytics, Smart Data und Cloud. Diese Aspekte führen in kleineren und mittelständige Unternehmen zu Verunsicherung. Weil die Aspekte implizieren, dass als erstes das herkömmliche Geschäftsmodell umgebaut und vollständig überdacht werden muss. Dafür müssen wichtige Handlungsschwerpunkte identifiziert und priorisiert werden. Zusätzlich rücken durch das wechselnde Markumfeld folgende Herausforderungen in den Vordergrund:
* Digitale Geschäftsplattformen werden immer beliebter werden Teil der Wertschöpfungskette. Unternehmen, die solche Systeme nicht verwenden um den Kundenzugang zu übernehmen können leicht von Wettbewerbern verdrängt werden.
* Die Digitalisierung muss erst einmal in dem Unternehmen umgesetzt werden. Was dazu führt, dass die bisherige IT eventuell mit der neuen nicht kompatible ist und es somit nicht gelingt Trends und Entwicklungen zu analysieren. 
* Der Umsatz bricht ein, weil die bisherige Produktpalette nicht attraktiv oder innovativ genug wirken.
* Ein Customer-Relationship-Management System wird immer wichtiger. Vertriebswegen ändern sich, Digitale-Distribution und Direktvertrieb ersetzen den Fachhandel.
* Das Unternehmen muss an Flexibilität gewinnen. Dafür ist die bisherige Unternehmensstruktur jedoch nicht ausgelegt und ein Mangel an Fachkräften kommt noch erschwert dazu. 

Wie klar zu erkennen ist geht das Verständnis was Digitalisierung darstellt stark auseinander und es sollte stets darauf geachtet werden, dass von einer einheitlichen Definition und Zielsetzung gesprochen wird.
[Informatik Spektrum]

Im Jahr 1941 wurde von Konrad Zuse und Helmut Schreyer der erste Computer entwickelt der Binärcode verwendet und programmierbar war. Der Zuse Z3 lautete das Digitale Zeitalter ein. Dreißig Jahre später wurde der erste Mikroprozessor mit 8.000 Transistoren patentier. Weitere zehn Jahre später 1981 waren es schon ca. 80.000 Transistoren. Der Chip-Entwickler AMD hat 2017 eine CPU mit 32-Kerne veröffentlich. Auf den 768 mm² des „AMD Epyc“ Chips befinden dank neuer 14 nm Fertigungstechnologie 19,2 Milliarden Transitoren. [https://en.wikipedia.org/wiki/Transistor_count]

Gorden Moore hat schon 1965 mit dem „Moore's law“ prognostiziert, dass die Anzahl der Transistoren ca. alle 18 Monate verdoppelt. In der folgenden Abbildung ist der Verlauf von 1971 bis 2016 visualisiert.

[https://en.wikipedia.org/wiki/Moore%27s_law#/media/File:Moore%27s_Law_Transistor_Count_1971-2016.png] [Wiki]

Ob und wie lange sich dieses Wachstum noch steigern lässt werden die nächsten Jahre zeigen. 

Ein weiterer interessanter Punkt ist wie eine derartige Beschleunigung des Fortschritts möglich war bzw. immer noch ist. Eine treibende Kraft ist die Digitalisierung die nicht nur die technischen und praktischen Anwendungsfelder verändert hat, sondern auch die Forschungs- und Entwicklungsarbeit. Ein Prozessor wir der AMD Epyc lässt sich nicht am Zeichenbrett planen. Es werden digitale Modellierungswerkzeuge benötigt und in der Produktion geht es ohne weitgehende Automatisierung und digitalisierten Verfahren zur Konstruktion nicht weiter. Dazu kommt das Optimierung und teils sogar Programmierung von Maschinen übernommen wird. Daher kann die Digitalisierung auch als hochdynamischer Prozess bezeichnet werden der sich sogar zum Teil selbst optimiert. Dazu kommen riesige Datensammlungen aus Fertigungsanlagen, Forschungsprojekten, Sozialen-Medien und dem alltäglichen Leben, die nur noch mit massiver bis vollständiger Unterstützung von Computern analysiert werden können. Auch wenn die Daten schwer zu analysieren sind kann dies zu ganz neuen Erkenntnissen führen, an die in manchen Fällen nicht einmal gedacht wurde oder sie waren vorher schlechtweg mit menschlicher Arbeitskraft nicht erreichbar. [Quelle Buch]

Ganz neue Möglichkeiten bringt da das aktuelle Hype-Thema der KI. Das s.g. „machine learning“ also Maschinelles Lernen (ML). Kurz erklärt, sammelt eine KI in einer kontrollierten Umgebung Erfahrungen, sie wird trainiert bzw. lernt, dann folgt eine Generalisierung um Probleme mit ähnliche Strukturen lösen zu können. Die KI erzeugt im Grunde Wissen aus den Erfahrungen. Ähnlich wie bei der Digitalisierung könnte auch hier ein exponentielles Wachstum entstehen. Dann wird von der gefürchteten „technological singularity“ oder „AI singularity“ gesprochen. Der Begriff steht dafür, dass Maschinen besser Maschinen als sich selbst entwickeln und dies in einem Iterativen-Prozess fortführen. Der eventuelle Ablauf und die Folgen für die Gesellschaft dienen als Grundlage für die Science-Fiction.

Der starke Bedarf an digitalen System wachst immer weiter. Alles und jeder soll mit dem Internet verbunden werden was zu der starken Dynamik in der Entwicklung führt. Die Anwendungsfelder erscheinen grenzenlos. Die Menschen wollen alles Verbessern und Optimieren. Dabei steht mehr Leistung, höhere Effizienz, geringerer Ressourcenverbrauch im Vordergrund und somit wird der enorme Entwicklungsschub angetrieben. Dieser ist so stark das berechtigterweise von der „digitalen Revolution“ gesprochen wird.

Wie schon gezeigt ist es schwierige eine vollumfassende Definition zu finde. Bei einer Formaldefinition müssen zur erfolgreichen Umsetzung in der Praxis einige Eigenschaften erfüllt werden. Dazu gehören die folgenden:
* Entscheidbarkeit: Bei einer Definition sollte es einfach erkennbar sein ob ein Phänomen als Digitalisierung beschrieben werden kann oder nicht.
* Umfassend: Die Akzeptanz der Definition spielt eine tragende Rolle bei ihrer Verwendung bzw. Verbreitung. Daher ist es wichtig, dass die Phänomene die in der Regel als Digitalisierung bezeichnet werden es auch weiterhin sind. Die Auswirkungen und Konsequenzen sind dabei nicht teil der Definition aber sollten sich aus der Definition ableiten lassen.
* Grundlage einer Skala: Für Unternehmen ist es wichtig das diese ihren Fortschritt bzw. Qualität im Bereich der Digitalisierung bewerten und vergleichen könne. Daher sollte es möglich sein eine Skala abzuleiten die zusätzlich die Best Practices einbezieht die sich wiederum aus der Definition ableiten lassen sollten. [Informatik Spektrum]

Mit diesen Anforderungen an die Definition lässt ich folgenden formulieren:

„Es wird von Digitalisierung gesprochen, wenn analoge Leistungserbringung durch Leistungserbringung in einem digitalen, computerhandhabbaren Modell ganz oder teilweise ersetzt wird.“ [Informatik Spektrum]

In [QUELLE] wird von Thomas Wolf und Jacqueline-Helena Strohschen [Quelle] zu der oben genannten Definition noch ein Reifegradmodell vorgestellt, welches dazu genutzt wird um Unternehmen hinsichtlich ihres Fortschritts im Bereich Digitalisierung zu bewerten. In dem Modell hat ein Unternehmen die maximale Reife erreicht, wenn alle potenziellen Möglichkeiten der Digitalisierung genutzt und im Zuge dessen die damit verbundenen Risiken vermieden werden. 

Es wird also von einer Analog-Digital-Wandlung (A/D-Wandlung) und Digital-Analog-Wandlung (D/A-Wandlung) gesprochen. Diese Umwandlung können ganz unterschiedliche Eigenschaften haben. So werden z.B. Messwerte in der Digitaltechnik mit analogen Sensoren erfasst und mit einem A/D-Wandler in digitale Größen übersetzt. Die A/D-Wandlung kann aber auch durch den Menschen durchgeführt werden. Wenn dieser z.B. eine Formel von einem Blatt in sein Excel-Dokument überträgt. Für eine D/A-Wandlung können z.B. Motoren als Akteure eingesetzt werden um etwas in der realen Welt zu verändern und wenn der Banker seine Formel ausdruckt findet ebenfalls eine D/A-Wandlung statt. Der wie oben definiert Begriff Digitalisierung lässt sich auf die „Objekte“ Produkte, Geschäftsprozesse, Wertschöpfungsprozesse, Humanressourcen, Aufgaben und Unternehmen anwenden, wenn diese eine Leistung erbringen. Dies Vorgang ist in der folgenden Abbildung verdeutlicht.

[Bild digitalisierungs_definition] [Informatik Spektrum]

Als ein weiteres Beispiel für die Digitalisierung wird häufig die Fotografie verwendet. Vor der Erfindung von Digitalkameras war die Fotografie ein photochemischer Prozess. Mit einer Digitalkamera wird ein digitales Abbild der im Sucher zu sehenden Wirklichkeit aufgenommen. An dem digitalen Abbild können vor dem Druck leichter Änderungen vorgenommen werden als an der analogen Aufnahme. Ein vollständiger Zyklus von A/D-Wandlung und wieder D/A-Wandlung kann in Verbindung mit andere Zyklen dazu verwendet werden um Begriffe wie Big Data und Data Analytics zu definieren bzw. abzuleiten. Aus der Verlauf von gekauften Produkten auf Plattformen wie Amazon lassen sich Kunden in Gruppe einteilen und typisieren. Die Benutzer Erfahrung wird mit gezielter Werbung und Angeboten gezielt gesteuert. Bei einer Fertigungsstraße werden Sammlungen von Sensordaten genutzt um einen Ausfall frühzeitig zu erkennen, dann wird von „predictive Maintenance“ gesprochen und eine entsprechende Wartung wird durchgeführt um den Schaden zu minimieren. [Informatik Spektrum]

## Die DNA der Maschinen

Immer mehr Maschinen werden programmiert, nur für die wenigsten Anwendungsfälle ist eine rein manuelle Bedienung noch ausreichend. Das liegt zum Teil daran das z.B. in großen Fertigungsstraßen die Abläufe so schnell und eng getaktet sind das ein Mensch gar nicht mehr mithalten könnte. Zusätzlich wird eine Vielzahl von gesammelten Sensordaten genutzt um die Umwelt zu erfassen. Des Weiteren im Quelltext hinterlegten Direktiven genutzt um auf die Umwelt zu reagieren. In der Umwelt agieren kann eine Maschine durch einen sogenannten Aktor z.B. einem Elektromotor, der Förderband beschleunigt. Die gesammelten Daten werden entsprechend abgespeichert und dienen als Grundlage für die Entwicklung von zukünftigen Maschinen.

In der Natur geschieht ähnliches. Organismen erfassen ebenfalls ihre Umgebung und reagieren entsprechend auf die gesammelten Daten indem sie diese interpretieren. Als Grundlage für einen Organismus dient der genetische Code (DNA), der in jeder Zelle des Wesens gespeichert ist. Die DNA enthält Wissen bzw. das Ergebnis der vorhergegangenen Evolution der Spezies. Im Idealfall können Organismus und Maschine durch die gesammelten Daten auch auf unbekannte Anforderungen reagieren. Bei Maschinen würde man dann von einer „artificial general intelligence“ (AGI) reden. Eine Künstliche Intelligenz, die also soweit Entwickelt ist, dass sie mit so gut wie allen Problemen umgehen kann und nicht auf wenige Anwendungsbereichen beschränkt ist. Der Mensch z.B. kann auf Grund seiner Erfahrungen und seinen erlernten Fähigkeiten auf unvorhergesehenes mit Intuition und Kreativität reagieren. Auch wenn auf den ersten Blick der Vergleich nicht ganz passend zu scheint zeigen sich viele Gemeinsamkeiten zwischen Maschinen und Organismen.
* Maschinen und Organismen speichern Informationen über Aktionen, Reaktionen und Eigenschaften einer bzw. ihrer eigenen Organisationseinheit. 
* Komplexe und Umfangreichen Informationen bzw. Daten werden mit wenigen unterschiedlichen Zeichen dargestellt. Bei der DNA reichen vier unterschiedliche Zeichen aus. Diese sind Nukleotiden-Adenin, Guanin, Thymin und Cytosin. Der heutige weitverbreitetste Binärcode von Maschinen besteht wie der Name schon sagt aus zwei Zeichen (0 und 1). (Exkurs: Modernste Quantencomputer verwenden sogenannte „Qubits“ die dem klassischen 0 und 1 schwer zu vergleichen sind. In der Vergangenheit wurde z.B. an Dezimalrechner geforscht die zehn unterschiedliche und nicht wie üblich nur zwei Signalstufen verwendeten.)
* Wie KI und aufwachsende Kindern zeigen sind beide Systeme grundsätzlich flexible und lernfähig. Selbst wenn sich dies in beiden Systemen stark unterscheidet. Ein Teenager besitzt eine weitaus höhere generelle Intelligenz als jeder Großrechner aber kein Mensch wird jemals schneller komplexe Rechenaufgaben lösen als ein Computer.
*  Nahezu verlustfreie Vervielfältigung von Informationen. Während der Zellteilung wird die DNA aufgespalten und mit einer komplementären Anlagerung wird eine neue DNA-Base erzeugt. Beim Menschen geht man davon aus das es eine Obergrenze von ca. 50 Spaltungen gibt, welche unsere Lebensdauer auf max. 130 Jahre einschränkt. Bei digitalen Informationen werden diese möglichst verlustfrei eingelesen und an anderer Stelle abgespeichert. Es werden Verfahren die der Hamming-Code verwendet um den Verlust von Daten bei der Übertragung bzw. Replikation zu vermeiden, wenn der Verlust jedoch zu groß ist eine Wiederherstellung nicht möglich.
* Informationen lassen sich bei Bedarf in beiden Systemen während der Vervielfältigung anpassen. Die DNA kann z.B. durch eine Mutation verändert werden wodurch die Nachkommen besser an den Lebensraum angepasst sind. Dieser natürliche Vorgang wird z.B. bei den sogenannten „Evolutionären Algorithmen (EA) in Software simuliert um Optimierungsprobleme zu lösen. Aber auch digitale Systeme, die keine EA verwenden können simple Datenmanipulation durchführen. (Exkurs: Bei einem EA wird eine zufällige Population an Lösungskandidaten für ein Problem generiert und anschließend mit einer Fitness-Funktion bewertet. Die Fitness-Funktion stellt die Angepasstheit des Individuums an das Problem bzw. die Umgebung fest. Die Individuen mit der höchsten Fitness werden für die Generierung der nächsten Population herangezogen. Dabei werden zwei Individuen durch eine Rekombination zusammenführt und das daraus resultierende Individuum wird durch eine zufällige Mutation nochmals leicht angepasst. Wiederholt man diese Schritte mehrmals wird die durchschnittliche Fitness der gesamten Population angehoben und man kann den fittesten Lösungskandidaten für das Problem herauspicken.)

Durch die gemeinsamen Eigenschaften können beide Systeme vorhandenes Anpassen und verbessern. Also es wird auf bereits Erreichtes als Basis verwendeten um darauf eine Veränderung durchzuführen. Durch dieses Vorgehen ist der enorme Innovationsschub der Digitalisierung zu erklären. Die Digitalisierung potenziert damit die Forschung und Entwicklung. Weiter verstärkt wird dieser Effekt dadurch, dass der Austausch von Daten so leicht ist. Die nächste Evolutionsstufe einer Technologie oder Software kann schnell und einfach umgesetzt werden und ist in Sekunden global verfügbar. Dieser Fortschritt wird allenfalls durch Patentstreitigkeiten und Politik eingeschränkt. [Quelle Buch]

### Der digitale Alltag

Im Jahr 2007 wurde vom Technologieunternehmen Apple das erste massentaugliche Smartphone auf den Markt gebracht. Das Konzept hat in den darauffolgenden Jahren das Leben der Menschen stark gewandelt. Auch Live-Streaming-Videoportal wie „Twitch“ und „YouTube“ und Instant-Messaging-Dienste wie „Discord“ und „TeamSpeak“ erlauben es uns von überall auf der Welt aus per Video oder Sprachkonferenzen Kontakte zu Mitmenschen aufzunehmen. Dies wirkt sich natürlich nicht nur auf das Privatleben, sondern auch auf das Berufsleben aus. Extra entwickelte Enterpreise Produkte wie „Skype for Business“ prägen die Berufswelt. Derartige Technologie hat sogar Auswirkungen auf das Mobilitätsverhalten der Menschen. Warum für ein Meeting treffen, wenn stattdessen eine „Web-Seminar“ gehalten werden kann. Dies spart Kosten und Zeit für alle Beteiligten. Selbst das Migrationsverhalten der Menschen beeinflusst die Digitalisierung. Wenn man immer und von überall aus all seine Verwandten und Bekannten kontaktieren kann fällt die Entscheidung ob man für ein lukratives Jobangebot umzieht oder nicht deutlich leichter. Eine solche Entscheidung muss sogar immer seltener getroffen werden. Viele Leute betreiben im „Home-Office“ und verlassen so zum Verdienen ihres Lebensunterhalts nicht einmal mehr ihr getrautes Heim. Informationen zu allem und jedem sind großenteils kostenlos im Internet verfügbar. Jeder kann selbst Fakten überprüfen und in Sekunden fehlendes Wissen nachschlagen. „Guides“ und „Tutorials“ helfen dabei ganz neue Fertigkeiten und Fähigkeiten für das Hobby oder Berufsleben zu erlangen. Zusätzlich werden durch die Digitalisierung sämtliche technischen Geräte so schneller weiterentwickelt und optimiert, dass man nur noch als Fachmann eine informierte Kaufentscheidung treffen kann. Somit kann gesagt werden, dass durch die Digitalisierung ist das gesamte Umfeld des Menschen im Umbruch. 

In der Industrie werden 3D-Drucker verwendet um individuelles Produktdesign zu ermöglichen und auch im privaten Bereich erlaubt ein 3D-Drucker die Erzeugung von Produkten. Privatpersonen können sich im Internet die CAD-Dateien herunterladen, die es ihnen erlauben Schusswaffen aus Kunststoff zu drucken. Über das Internet können nicht nur Waren verbreitet werden, sondern auch Inhalte. In Sekundenschnelle lassen sich selbst erstellte Bilder, Bücher, Filme, Musik, Meinungen, Ideen, Gegenstände und Programme verbreiten. Dabei brauch man nicht einmal einen Verlag oder eine andere Instanz, die einen eventuell kontrolliert oder zensiert. Trotz der Verbote werden natürlich auch illegale oder urheberrechtlich geschützt Inhalte online gestellt. Häufig ist es schwer oder gar unmöglich derartige Inhalte wieder aus dem Netz zu bekommen. Gesetze und Urheberrecht sind nicht global gültig und selbst Inhalten, die in praktisch allen Staaten der Erde verboten sind alles sich nur schwer eindämmen oder entfernen. Es wird häufig davon geredet, dass das Internet ein rechtsfreier Raum sei was natürlich nicht stimmt. Was allerdings stimmt es, dass die Durchsetzung vieler Gesätze deutlich erschwert ist. Somit entstehen wirtschaftliche und soziale Gefahren mit denen wir nicht umzugehen wissen und dies noch lernen müssen. 

Mit den immer weiterentwickelten Technologien steigen auch die Erwartungen und der Anspruch. Der Mensch gewöhnt sich unheimlich schnell an den zusätzlichen Komfort die eine Technologie mit sich bringt. Jedoch ist der Mensch auch genauso schnell davon gelangweilt und wünscht sich neue Inhalte und Weiterentwicklung. Dadurch wird die Wirtschaft in schneller Produktentwicklung und kürzere Produktlebenszyklen gezwungen.

Vorteile bringt dies jedoch auch mit sich. Die Digitalisierung übt eine antreibende Wirkung auf den Arbeitsmarkt aus. Häufig wird behauptet das Digitaltechnik den Menschen ersetzen würde und somit die Arbeitsplätze zerstört. Jedoch wird die Funktionen der Menschen durch diesen Effekt verändert, die Tätigkeitsprofile der Mitarbeiter werden angepasst. Insgesamt betrachtet wächst das Erwerbsangebot sogar. Was stimmt ist das heute mehr Flexibilität und Lernbereitschaft verlangt wird. Um auf die dynamischen Marktveränderungen reagieren zu können muss das Unternehmen selbst flexible sein und mit neuen Produkten und disruptiven Geschäftsmodellen punkten. Der Bedarf der Märkte muss schnell erkannt und am besten sogar vorhergesagt werden um nicht nur mit passenden Produkten reagieren, sondern auch agieren zu können. 

Dies setzt natürlich verschiedenste Eigenschaften bei den Mitarbeitern voraus. Dazu gehört das lebenslange Lernen, das in diesem Zusammenhang für alle am wirtschaften Prozess Beteiligen zur unvermeidlichen Realität geworden ist. Dies haben wir der Dynamisierung der Entwicklung durch die Digitalisierung zu verdanken. [Quelle Buch]

### Anforderungen an Digitaltechnik (Verlässlichkeit und Effizienz)

Die Anforderungen an Digitaltechnik sind hoch und dies ist auch gut so weil sie inzwischen in so gut wie allen Bereichen zu finden ist. Digitaltechnik wird genutzt um technisch die Wirtschaft, Wissenschaft, öffentliches und privates Leben zu steuern kontrollieren und überwachen. Essenzielle Bereiche wie Sicherheit, Gesundheit, Energieversorgung, Produktion, Mobilität, Kommunikation und Median werden so betrieben. Der breitgefächerte Einsatz sorgt für die hohen Anforderungen. Hinsichtlich der Effizienz von Systemen wurden in den letzten Jahren große Fortschritte gemacht. Dabei setzt sich die Effizienz aus unterschiedliche Aspekten zusammen.
* Energieverbrauch: Elektroartikel werden immer sparsamer. Dieser Trend wird durch den Umweltschutz und Kostenreduktion vorangetrieben. Bei mobilen Geräten und Wearables ist natürlich eine möglichste lange Akkulaufzeit wünschenswert. Durch die schon angesprochenen neuen Fertigungstechniken bei der CPU Fertigung wurde nicht nur die Anzahl der Transistoren enorm angehoben, sondern auch der Energieverbrauch gesenkt bzw. trotz Leistungssteigerung im herkömmlichen Bereichen gehalten.
* Codegröße und Speicherplatz: Der Source-Code von Programmen wird häufig in Richtung geringer Laufzeit optimiert jedoch auch auf die Größe wird geachtet. Niemand möchte eine Festplatte für sein Smartphone benötigen. Weil diese anfällig gegenüber Stößen und Erschütterungen ist. Des Weiteren ist allgemein die Datendichte auf unterschiedlichsten Datenträgern stark angestiegen während die Preise gefallen sind. Vor wenigen Jahren war eine herkömmliche SSD (Solid-State-Drive) ein Luxus für Privatpersonen und inzwischen gibt es Laptops die gar keine HDD (Hard-Disk-Drive) mehr verwenden. Durch diesen Fortschritt können tausende von Bilder, Musikstücken und Dateien z.B. auf einem simplen USB-Stick gespeichert werden.
* Laufzeit: Niemand wartet gerne. Warten ist Zeitverschwendung und in der Wirtschaft ist wie allseits bekannt Zeit mit Geld aufzurechnen. Außerdem wird mit geringeren Laufzeit automatisch der Energieverbrauch gesenkt. Aber auch für den Aspekt der funktionalen Sicherheit ist dieser Punkt extrem wichtig. In Systemen in denen harte Echtzeitbedingungen eingehalten werden müssen hängen davon ggf. Menschenleben ab. Wenn eine harte Zeitbedingung verletzt wird ist dies als ein Fehler zu betrachten, z.B. Lenkung eines Automobiles. Bei weichen Echtzeitbedingung wie z.B. einem Videodecoder eines Streams ist es lediglich wichtig das die Zeitbedingung im statistischen Rahmen liegt. Kurzzeitiges ruckeln des Stream gefährden keine Menschenleben oder die Unversehrtheit von Maschinen in Produktionsstraßen.
* Gewicht und Größe: Allgemein wird Elektronik immer kleiner und damit auch leichter. Früher waren Computer so groß wie ganze Räume und wogen Tonnen und nicht ein paar Gramm wie heutige Prozessoren. Dies führt auch dazu das Geräte leicht tragbar sind und somit auch leicht verbaubar. Besonders wichtig ist dieser Aspekt bei Alltagsgegenständen wie Smartphone, Smartwatch oder Wearables die ständig direkt am Körper mitgeführt werden. Wenn z.B. Satelliten von Rakete in den Orbit befördert werden zählt jedes Gramm um den erforderten Treibstoff zu reduzieren.
* Preis: Der häufig wichtigste Punkt bei einem System. Er beeinflusst in der Regel alle anderen Aspekte. Generell kann gesagt werden, dass der Preis von Elektronikartikeln stark gesunken ist. Vor allem wurde das Preis-Leistungs-Verhältnis verbessert. Ein einzelnes Smartphone besitzt zehnmal so viel Leistung wie ein 25 Jahre alter Computer. [Quelle ES Buch]

Es existieren noch weitere Anforderungen, die hinsichtlich der Verlässlichkeit von Systemen betrachtet werden.
* Zuverlässigkeit: Viele Systeme im Umfeld des Menschen dürfen schlichtweg nicht ausfallen. Um dies zu realisieren gibt es drei generelle Möglichkeiten. Es werden Ersatzkomponenten bereitgestellt, die bei einem Ausfall die Funktion übernehmen. Beispielsweise ein Notstromgenerator für ein Rechenzentrum. Eine weitere Möglichkeit ist die Verwendung von parallelen arbeitende Komponenten, welche die zusätzliche Belastung im Fehlerfall übernehmen können. Beispielsweise Mehrstrahliges Flugzeug. Als dritte Möglichkeit können verschiedene Prinzipien genutzt werden um eine Funktionalität abzusichern. Dies könnte z.B. durch die Verwendung von elektronischen und mechanischen Messgeräten realisiert werden. Dazu kommt, dass z.B. in einem Flugzeug mehrere Systeme Werte wie Flughöhe etc. ermitteln und dort ein Mehrheitsentscheid durch die Systeme getroffen wird, wenn die Ergebnisse der Systeme sich unterscheiden.
* Wartbarkeit: Falls es mal bei hoch kritischen Systemen zu einem Ausfall kommt sollten sich das Problem schnell ermitteln lassen und daraufhin auch schnell reparieren lassen. Wenn z.B. die Server-Systeme einer Bank nicht funktionstüchtig sind lähmt dies das gesamte Unternehmen was im schlimmsten Fall zur Insolvenz führen kann. Es sollte auch darauf geachtet werden, dass das System von Benutzern mit unterschiedlichem Wissenstand bedient bzw. gewartet wird. Für den Austausch einer Grafikkarte im Desktop-Computer sollte man also kein Elektrotechnikstudium benötigen. 
* Verfügbarkeit: Eine hohe Verfügbarkeit setzt sich aus den Aspekten Wartbarkeit und Zuverlässigkeit zusammen. Je besser diese beiden Aspekte sind um so höher ist die Verfügbarkeit eines Systems.
* Funktionale Sicherheit oder Betriebssicherheit (Safety): Wie schon bei dem Aspekt der Laufzeit angesprochen ist die funktionale Sicherheit ein wichtiger Punkt. Es sollten keine Schäden an Mensch oder Maschine entstehen, wenn ein System ausfällt oder ein Fehler auftritt. Leider hört man jedoch immer noch an Unfällen in den Menschen im Alltag durch Maschinen wie automatischen Drehtüren verletzt werden. Grade mit dem aktuellen Thema des automatisierten Fahrens steht dieser Punkt im Fokus. 
* Integrität (Security): Die Gewährleistung von Daten- und Kommunikationssicherheit ist ein weiterer extrem wichtigerer Punkt. Eng verknüpft mit dem Datenschutz und der Privatsphäre. Niemand hört gerne das groß Konzerne die Chat-Nachrichten oder E-Mails, die man an seine Freunde und Familie schickt mitlesen, analysieren und kategorisieren. Damit anschließend, im harmlosesten Fall, maßgeschneiderte Werbung geschaltet werden kann. [Quelle ES Buch]

Am Beispiel des automatisierten Fahrens lässt sich die Abhängigkeit von Digitaltechnik gut verdeutlichen. Damit bedenkenlos die Hände vom Lenkrad genommen werden können müssen riesige Datenmengen erfasst und ausgewertet. Eine Vielzahl von Steuerungsgeräten muss verlässlich miteinander kommunizieren und interagieren. Dazu kommen dann noch Daten die das Auto selbst nicht erfasst, sondern nur empfängt und nutzt. Beispielsweise GPS-Positionsdaten über Stellite und Informationen anderer autonom fahrender Fahrzeuge. Diese Abhängigkeit zieht sich durch sämtliche Bereiche. Beispielweise Informations- und Kommunikationstechnik, Gesundheitstechnik, Logistik und Sicherheitssysteme. Die Digitaltechnik dient somit als fundamentaler Bestandteil der technisch orientierten Zivilisationen.

Grade die beiden oben beschrieben Aspekte Safety und Security stellen große Herausforderungen und Probleme dar und deshalb lassen sie sich als Kernthemen der Digitalisierung festgelegen. Dafür muss darauf geachtet werden, dass die Systeme so gestaltet werden, dass sie ausnahmslos im Interesse der Menschen funktionieren, agieren und reagieren. In der heutigen Welt werden praktisch alle Informationen digital gespeichert und transportiert. Dabei werden diese Daten nicht nur von den Menschen konsumiert, sondern auch erzeugt bzw. generiert. Auch Maschinen erzeugen Daten während sie im Einsatz sind und ermitteln ständig ihre Umgebung. In den letzten Jahren ist die Anzahl der Daten immer weitergewachsen und es scheint auch kein Ende in Sicht. Selbst wer aktiv versucht keine digitalen Spuren zu erzeugen kann sich kaum davor Schützen. Es ist praktisch unmöglich in der westlichen Welt zu leben ohne das Unternehmen und Staat Profile zu einem anlegen oder in der Lage dazu wären. Dies geschieht schon durch Kameras in der Öffentlichkeit und Überwachungssysteme. Auch besitzt praktisch jeder Bürger ein Bankkonto wo das Gehalt hin überwiesen wird oder ein Auto das eine Versicherung benötigt und ein Smartphone, mit dem im schlimmsten Fall die Privatsphäre regelrecht gläsern werden kann. Von großen Sozialen-Medien werden sogar angeblich Profile zu Personen erzeugt, die nicht einmal Benutzer der Plattform sind. Dies passiert z.B., wenn ein Foto hochgeladen wird das eine Person zeigt, die nicht Benutzer der Plattform ist. Solche Praktiken verstoßen jedoch zum Glück gegen geltendes Recht was sich aber schwer durchzusetzen ist. Der artigen personenbezogenen Daten bzw. Informationen besitzen wirtschaftlichen Wert und damit wird folglich auch gehandelt. Sie gehören wie auch die nicht personenbezogene Daten heute schon zu den wichtigsten Waren des 21. Jahrhunderts. Man spricht sogar vom s.g. „Informationszeitalter“. Angriffe auf die Security von IT-Systemen werden immer ausgefeilter und mit wachsender Komplexität von Systemen wird es auch immer schwieriger sich vor solchen zu schützen. Es besteht also im Punkto Sicherheit ein permanenter Forschungsbedarf. Grade bei Cyberangriffen und Maßnahmen gegen diese handelt sich um ein ständiges Wettrüsten. 

Daraus folgt, dass Mitarbeiter und Fachleute in solchen Bereichen das schon angesprochene lebenslange Lernen in ihrem Berufsleben ausüben müssen. Auch muss bei den Beteiligten ein ausgeprägtes Sicherheitsbewusstsein entstehen. Grade im Bereich des Datenschutzes und der digitalen Sicherheit ist es wichtig auch in der Gesellschaft und Politik eine Sensibilisierung durchzuführen. In Zukunft sollte es genau so selbstverständlich sein, dass man seine Daten schützt wie das Abschließen des Autos oder der Haustür. Je stärker eine Technologie dezentralisiert wird umso mehr Verantwortlichkeit liegt bei den Anwendern. Dabei bezieht sich die Verantwortlichkeit nicht nur auf Sicherheit, sondern auch auf die ethischen Hintergründe. [Quelle Buch]

## Anwendungsbereiche der Digitalisierung 

Wie in der Einleitung für dieses Kapitel bereits klar geworden ist gehört die Digitalisierung bereits zum alltäglichen Leben. Im Grunde steht die Digitalisierung für die digitale Repräsentierung von Informationen und Aspekten von physischen Objekten sowie deren Analyse, Kreierung und Verarbeitung. Dabei fällt auf das sie als eine Art „Universalübersetzer“ genutzt wird um Daten und Informationen unterschiedlichsten Ursprungs für Maschinen bearbeitbar zu machen. Dadurch entstehen vorher undenkbare Möglichkeiten für z.B. Simulationen von Maschinen, Prozessen, Objekten und sogar ganzen Systemen in Verbindung mit komplexen Analysen können Problemstellungen ganz anders angegangen werden. Bei einem, sogenannten Brain-Computer-Interface (BCI) z.B. werden menschliche Gehirnsignale mit Sensoren auf der Kopfhaut erfasst und können so dazu genutzt werden um mit Computern und Robotern zu interagieren. Auch anders herum ist es möglich, haptische Empfindungen können an Prothesen mit digitalen Signalen erzeugt werden. Solche Technologien sind heutzutage für körperliche behinderte Personen ein Segen und in Zukunft wird der Anwendungsbereich sicherlich nicht kleiner. Beim BCI wird die Digitalisierung als direktes Verbindung zwischen dem Körper (biologisch) und Computern, Maschinen und Prothesen (cyber-physisch) genutzt. [Quelle Buch]

### Der Universalübersetzer im Einsatz

Die materielle Welt wird immer detailgetreuer digitalisiert. Damit Objekte korrekt in der digitalen Welt rekonstruiert werden können, müssen diese analysiert werden. Dabei ist man auf die Informationen über das Objekt aus. Die Zusammensetzung, Aufbau und die Form der Informationen liegen im Fokus. Diese Eigenschaften bzw. Parameter können digital abgebildet werden und im Computer-Model für das Objekt genutzt werden. So können z.B. komplexe Systeme, Maschinen, Materialien und z.B. Medikamente am Computer geplant werden. Anschließend können diese in unterschiedlichsten Simulationen auf Aspekte wie Tauglichkeit geprüft werden. 

Virtual-Reality-Projektionen stellen digitale Welten dar und mit 3D-Kameras oder anderen Tracking-Systemen kann der Benutzer mit dem System interagieren und direkt die Simulation nach Belieben manipulieren.

Google-Street-View ist ein perfektes Beispiel für die Digitalisierung der materiellen Welt. Mit dem Browser kann jedermann die Straßen der Welt besuchen und diese aus dem Blinkwinkel eines Reisebus Passagiers betrachten. Dafür werden mit Autos, die mit 360 Grad Kameras ausgestattet sind durch die Straßen gefahren und permanent Bilder aufgenommen. Diese Fotos werden anschließend intelligent und mit Hilfe von KI zusammengeführt. Das Ergebnis ist, das man sich von Zuhause aus per Knopfdruck, durch die Straßen des geplanten Urlaubortes klicken kann ohne auch nur das getraute Heim zu verlassen. Inzwischen wurde diese Technologie noch ausgeweitet und dahingehend verbessert, dass für viele Großstätte sogar ein 3D-Modell angezeigt werden kann. Derartigen Projekte existieren auch für Museen und viele andere Sehenswürdigkeiten die dann z.B. auch in Virtuellen-Realitäten betreten werden können. Dadurch sind Kulturgüter jedem zugänglich und für die oftmals sehr wertvollen Originale besteht nicht die Gefahr, dass diese entwendet oder beschädigt werden. Viele Ausstellungstücke sind sehr empfindlich was z.B. Licht oder den Kontakt mit Sauerstoff angeht, so könnten diese bei falscher Aufbewahrung bzw. Ausstellung korrodieren oder ausbleichen. Die Digitalisierung von solchen Objekten ist schon alleine der Redundanz wegen sinnvoll. Es kann immer einen Brand, Diebstahl oder eine Überschwemmung geben, die das Objekt beschädigen oder sogar zerstören. 

Währen der Erstellung dieses Buches wurden von den Autoren des Buches zwei Projekte umgesetzt. Eines von diesen ist ebenfalls ein gutes Beispiel für die Digitalisierung der materiellen Welt. Das andere wird in Verbindung mit zusätzlichen Themen im nächsten Abschnitt erläutert. Bei dem Projekt „VINI – Vehicle Identification Number Index“ handelt es sich um die Digitalisierung des klassischen gedruckten und händisch geführten Scheckheftes für Kraftfahrzeuge. Ziele sind unter anderem die Manipulation von Tachoständen und Scheckheft Einträgen zu verhindern. Dafür werden einige Eigenschaften einer Blockchain verwendet um Daten zu einem Kfz dauerhaft, unveränderlich und dezentral zu speichern. Wichtig ist dabei auch, dass nachvollziehbar wird wer wann welchen Eintrag erstellt hat und so eventuelle Manipulationsversuche offensichtlich werden. Mehr zu dem Projekt ist unter (https://github.com/SGSE18/VINI/blob/master/pflichtenheft/VINI%20Pflichtenheft.pdf) zu finden.

### Verbesserung der Medizin

In der Medizin fallen riesige Datenmengen über die Patienten an. Dazu gehören z.B. Bildaufnahmen, Textinformationen, Elektrokardiogramme (EKG), Röntgenaufnahmen und MRT-Scans. Diese effizient, sicher und effektiv zu analysieren ist eine Herausforderung und in menschlicher Zeit kaum noch möglich. Dazu kommen dann sogar noch Laborbefunde und Informationen aus der Fachliteratur die berücksichtig werden sollten. Um dieses Problem zu lösen wird inzwischen an KI unterstützten Diagnose Techniken geforscht. Neuronale-Netze, die mit Scans von tausenden von Patienten trainiert werden können schon jetzt in vielen Bereichen hilfreich sein und übertreffen sogar in einigen Bereichen die Erkennungsraten der Ärzte. In der Chirurgie werden inzwischen ferngesteuerte Roboter genutzt damit Fachärzte sich lange und kostspielige Reisen queer über den Globus ersparen können. Dabei wir die „Telerobotik“ verwendet um eine „Teleoperation“ durchzuführen. Dabei bedeutet dies lediglich „Arbeit auf Distanz“. Derartige Telerobotik wird auch zur Erforschung der Tiefsee genutzt oder um Bohrinseln zu reparieren. So können auch Bomben sicher aus großer Entfernung sicher entschärft werden auch können so radioaktive Stoffe verarbeitet werden ohne das Leben von Menschen zu gefährden. [Quelle Buch] 

Wie im vorherigen Abschnitt schon angeschnitten wurde von den Autoren dieses Buch ein weiteres Projekt umgesetzt. Dieses ist eng mit dem den oben genannten Daten eines Patienten verbunden. Genauer dreht es sich bei dem Projekt namens „Health Ledger: Die digitale Patientenakte“ um die Krankenakte von Patienten. Das Projektteam hat erkannt, dass die Krankenakte einen besonders hohen Schutz benötigt. Die Krankenakte als Informationssammlung bietet tiefe Einblicke in die Privatsphäre und ist im jetzigen Gesundheitssystem häufig nicht ausreichend geschützt und der Patient selbst besitzt Zuwenig Kontrolle über diese empfindlichen Informationen. Es ist für Patienten häufig nicht nachvollziehbar, wer wann und wie lange Einsicht bzw. Zugriff hat. Das Projekt versucht diese Mängel zu beheben in dem der Patient selbst die vollständige Kontrolle über seine eigenen Krankendaten besitzt und anderen Zugriff und Einsicht gewähren kann, wenn dies erwünscht ist. Das System ist dezentral über eine Blockchain organisiert und mit kryptographischen Verfahren wird die Integrität der Daten sichergestellt. Das Projekt ist unter (https://github.com/SGSE18/health-ledger/blob/master/pflichtenheft/README.md) zu finden.

### Datenkomprimierung und digitaler Rundfunk

Die Menge an jährlich erzeugten digitalen Daten wächst exponentiell an. Die Digitalisierung treibt dies maßgeblich voran. Der Ausbau der Infrastruktur kommt den ständig wachsenden Anforderungen nicht nach. Engpässe und Internet-Drosseln sind die Folge. Hochrechnungen zu folge soll im Jahr 2025, die global erzeugte Datenmenge des Jahrs 2016, verzehnfacht werden. Es wird von Zahlen im 163 Zettabyte Bereich geredet, dies entspricht in etwa 41.000 Mrd. DVDs.

Streaming von Musik und Video nehmen einen großen Teil der Bandbreite in Anspruch. Entsprechend wichtig wird die Datenkomprimierung um den Traffic zu reduzieren. Natürlich kommt dies auch dem Heimcomputer oder Smartphone zu gute. Besonders wichtig sollte die verlustfreie Datenkompression sein. 

Beim Video-Kodierer „H.265“ war es z.B. das Ziel die Qualität von H.264 beizubehalten aber die Kompression zu verdoppeln. Die weltbekannte und weit verbreitete mp3-Kodierung von Audiodateien kann bei gleicher Klangqualität die Dateigröße von bis zu um einen Faktor Zehn reduzieren. Ohne derartige Verfahren würde das Anschauen eines einzelnen YouTube-Videos praktisch unmöglich sein. Die benötigte Bandbreite wäre gigantisch was Netzwerke und Datenvolumen vollständig auslasten würde.

Auch der digitale Rundfunk profitiert von solcher Technologie. Der digitale Rundfunk selbst bietet viele Vorteile. Zum einen kann er via Funk (terrestrisch) empfangen werden und ist abgesehen von dem Rundfunkbeitrag kostenlos. Er ist dabei unabhängig vom Internet und bietet eine energieeffiziente Übertragung, die dazu auch noch störungsfrei und von hoher Klangqualität ist. Auch lassen sich so mehr Sender als vorher realisieren, da das Funksignal schlanker ist. Ein weiterer wichtiger Vorteil ist, dass über digitalen Rundfunk mit hoher Zuverlässigkeit und Reichweite Warnmeldungen über Katastrophen und Verkehrsprobleme verschickt werden können. Es ist davon auszugehen, dass in den kommenden Jahren immer mehr analoge Rundfunksysteme mit digitalen ersetzt werden. Dieser Trend ist schon in Schwellenländern wie z.B. Indien zu erkennen. [Quelle Buch]

### Je schneller, desto besser!

Eine schnelle Datenübertragung ist in vielen Anwendungsfällen wünschenswert. Häufig wird jedoch der Begriff „schnell“ mit hoher Bandbreite gleichgesetzt. Die Menschen reden davon, dass sie „schnelles Internet“ besitzen, wenn sie jedoch eigentlich davon sprechen, dass die Bandbreit hoch ist. Das „schnell“ sollte sich jedoch auf die Latenzzeit (umgangssprachlich „Ping“) der Verbindung beziehen. Also die Bandbreite beschreit wie hoch der Datendurchsatz ist und die Latenz die Wartezeit zwischen Anfragen. Wer schon einmal einen Internetzugang über Satellit genutzt hat weiß, dass die Bandbreit durch aus akzeptable ist jedoch die Latenzzeit viele Anwendungsfäll unmöglich macht. Die hohe Latenzzeit stammt in dem Fall natürlich daher, dass bei jeder Anfrage das Signal von der Erdoberfläche bis zum Satelliten geschickt werden muss. Die Entfernung ist so groß, dass obwohl das Signal mit Lichtgeschwindigkeit transportiert wird die Verzögerungen deutlich spürbar wird. Der unterschied wird noch klarer, wenn das Internet zur Zeit der 56k Modems betrachtet wird. In zeitkritischen Anwendungen wie z.B. Onlinespiel entscheidet die Latenz darüber wie lang es dauert bis ein Gegenspieler angezeigt wird. Also wie schnell auf eine Aktion z.B. durch einen Server reagiert wird. Wenn der Sever des Spiels nicht tausende von Kilometern entfernt stand, wahren auch früher geringe Latenzzeiten möglich. Bei zu hoher Latenz werden Onlinespiele unspielbar und die geringe Bandbreite der alten Modems hat eine Datenübertragung von mehreren Gigabyte zu einer tagelangen Aufgabe gemacht. Somit war z.B. Streaming von Videos und Musik unmöglich, weil die Bandbreiter schlichtweg nicht vorhanden war. Vom Fraunhofer Institut wird der Bereich des Internets, welcher von minimalen Verzögerungszeiten abhängig ist als „Taktiles Internet“ bezeichnet.

Wichtig ist diese Unterscheidung, da eine geringe Latenzzeit die Grundlage für eine Vielzahl neuer Anwendungen ist. Autonom fahrende Autos, automatisierte und vernetzte Maschinen und die schon angesprochenen Teleroboter sind z.B. davon abhängig und mit zu hoher Latenz nur schwer oder gar nicht umsetzbar. Es ist davon auszugehen, dass der Anteil von Daten, die durch Maschinen bzw. Sensoren erzeugt werden in den nächsten Jahren immer weiterwachsen wird. Diese Daten stammen aus dem Industriebereich und auch aus dem wachsenden Trend des „Internet of Things“ (IoT). Die Begriffsabgrenzung ist ähnlich umständlich wie bei der Digitalisierung selbst. Häufig wird IoT als einen Sammelbegriff für unterschiedlichste Technologien der globalen Informationsgesellschaft, die es möglich macht, physische und virtuelle Gegenstände/Maschinen miteinander zu vernetzen damit diese Informationen untereinander austauschen und zusammenarbeiten. In der Industrie spricht man auch von dem Industrial Internet, welche hohen Anforderungen an solche Technologien stellt. Es wird nach harter und weicher Echtzeitfähigkeit verlangt und auch nach Skalierbarkeit, Interoperabilität und Datenintegrität. Der Mobilfunk spielt hier auch eine stätig wachsende Rolle. Der 5G-Mobilfunkstandard soll mit bis zu 10 Gbit/s zehnmal schneller als 4G/LTE sein. Zusätzlich wird auf Cloud-Computing und Edge Computing gesetzt. Beide Themen werden in den entsprechenden Kapiteln ausführlich erklärt. Beim Edge Computing werden unter anderem zur Verkürzung von Latenzzeiten, Daten direkt auf den Maschinen oder Sensorknoten verarbeitet damit die langen Latenzzeiten der Cloud nicht berücksichtig werden müssen. [Quelle Buch]

## Arbeit und Produktion

### Digitale Arbeits- und Produktionswelten

Die Arbeitswelt wurde durch die Digitalisierung bereits stark verändert und sie wird es auch noch weiter tun. Inzwischen ist es so weit das man seine Steuererklärung nur noch nach Beantragung eines Härtefallantrags per Papier beim Finanzamt abgeben kann. Auch die Alltagskommunikation im privaten wie auch im beruflichen Bereich findet hauptsächlich nicht mehr über Briefe statt sondern über E-Mails oder Chat-Programme. Dieser Trend ist nicht mehr zu stoppen und wird sich in den nächsten Jahren noch verstärken. Versuchen Sie mal einen Beruf zu finden in dem nicht mit Digitalisierung gearbeitet wird. 

Prototypen von geplanten Produkten werden nicht mehr am Zeichenbrett, sondern am Computer geplant und entworfen. Auch wenn die interaktiven digitalen Assistenzsysteme noch in ihren Kinderschuhen stecken ist davon auszugehen, dass diese in Zukunft immer stärker genutzt werden. Zusätzlich wird an Robotern geforscht, die den Menschen direkt bei seiner Tätigkeit unterstützen und z.B. einen dritten Arm oder eine Hilfskraft simulieren. Unterschiedlichste Tracking-Systeme helfen dabei, dass keine Mitarbeiter verletzt werden. Parallel wird an Sprach-, Gesten- und Emotionserkennung geforscht um die Interaktion mit Maschinen so intuitiv wie möglich zu gestalten. Wichtig bei dieser engen Zusammenarbeit ist es, dass die Interaktionsschnittstellen auf die Menschen zugeschnitten ist und nicht durch technische Limitierungen bestimmt wird. Ansonsten führt dies zur Entstehung unsichere oder nicht nutzerfreundlicher Systeme. 

Maschinen die aus einer Kombination von mechanischen, elektronischen und digitalen Komponenten bestehen, die auch noch über das Internet kommunizieren können, werden „Cyber-physische Systeme“ (CPS) genannt. Diese stellen die Basis für das nächste große Schlagwort „Industrie 4.0“ (I4.0) da. Ebenso wie bei dem Internet of Things oder der Digitalisierung sind diese Begriffe ebenso uneindeutig definiert. In der Regel spricht man von Industrie 4.0 bei Produktionsanlagen oder Produktionssystemen die durchgehend vernetzt sind. Vernetzt werden dann z.B. die oben schon genannten digitalen Assistenzsysteme, kooperierende Robotersysteme, Steuergeräte und natürlich Computer, alle genannten Geräte werden durch Netzwerke oder das Internet verknüpft.

Ein beliebtes Konzept ist dabei der „digitale Zwilling“ oder auch „digital Twin“ genannt. Dieser stellt eine digitale Kopie von Maschinen und auch Gütern dar. Die digitale Repräsentierung sollte dabei über alle betriebsrelevanten Eigenschaften verfügen. So können z.B. Instandhaltungsstrategien wie „predictive maintenance“ umgesetzt werden. Dabei geht es darum, dass frühzeitig gewarnt wird, wenn z.B. Verschleißteile an ihre Grenzen kommen. Dies erlaubt eine frühzeitige Wartung der Maschine und senkt eventuelle Kosten. Mit dem digitalen Zwilling lässt sich aber auch nach Optimierungsmöglichkeiten und potenziellen Fehlern in der Produktion suchen. In Simulationen können so verschiedene Bedingungen simuliert werden und das daraus resultierende Verhalten der Maschine getestet und bewertet werden. Allgemein kann gesagt werden, dass die Industrie 4.0 dazu beitragen soll, Prozesse ressourceneffizient zu planen und zu optimieren. Auch soll die Industrie 4.0 dazu beitragen die Arbeitsbedingungen zu verbessern und günstig individualisierte Produkte zu realisieren. [Quelle Buch]

### Generative Fertigung

Das Gegenstück zur Digitalisierung der materiellen Welt ist die generative Fertigung bei der Daten in Materie umgewandelt werden. Das offensichtlichste Beispiel ist der „3D-Druck“. Digitale gestaltete Objekte könnte damit in die reale Welt „materialisiert“ werden. Einer der Vorteile ist, dass die Daten vorher nach belieben bearbeitet und dann verschickt werden können. In der Einleitung wurde ein Vergleich von DAN und Daten geschaffen der hier auch genutzt werden kann. Die Daten bzw. Informationen zu einem Objekt bestimmen wie die DAN die physische Erscheinung. 

Inzwischen kann man immer präziserer „drucken“ und die Materialvielfalt und Qualität nimmt stetig zu. Mit 3D-Druckern lassen sich preiswert Prototypen entwickeln und Spezialanfertigungen können leichter umgesetzt werden. Zusätzlich können in der Medizin Prothesen für jeden einzelnen Patienten angefertigt werden, die genau auf dessen Probleme abgestimmt sind. Auch wenn der 3D-Druck aktuell noch vergleich bar langsam ist bietet er oftmals eine höhere Materialeffizienz. Das Produkt wir direkt aus der Materialmasse gefertigt und nicht z.B. aus einem Block Rohmaterial heraus gefräst. Bei filigranen und vorm trocken statisch instabilen Objekten wird jedoch oft ein Füllmaterial genutzt um die Konstruktion zu stützen. Beispielsweise wird das Material direkt in einem gelatineartigen Gel gedruckt, welches die Konstruktion während des Prozesses permanent von allen Seiten umgibt und somit stabilisiert. Die generative Fertigung soll auf Grund der Vorteile, die sie bietet fester Bestandteil des Industrie 4.0 Konzept werden.

Inzwischen wird sogar mit 3D-Druckern auf der Internationale Raumstation (ISS) gearbeitet. Mit genug Rohmaterial könnten bei Bedarf direkt vor Ort Ersatzteile produziert werden, die dann nicht umständlich von der Erde aus transportiert werden müssten. Wenn das ISS Szenario auf die Spitze getrieben wird und Objekte sich Atom für Atom genau einscannen lassen und wieder ausdrucken lassen könnte man damit quasi einen Teleporter realisieren der jedes Mal eine exakte Kopie des Objekts erzeugt. [Quelle Buch)

### Künstliche Intelligenz und kognitive Maschinen als Freund und Helfer

Künstliche Intelligenz spielt jetzt schon in vielen Bereichen eine große Rolle. Der nächste Schritt ist jedoch die sogenannte „artificial general intelligence“ (AGI) oder „kognitive Maschine“. Diese Systeme sollen über Interaktionsfähigkeit, Erinnerungsvermögen, Kontexterfassung, Anpassungsfähigkeit und Lernfähigkeit verfügen. Derartige Maschinen würden den Turing-Test bestehen und es damit für den Menschen nicht mehr unterscheidbar machen ob es sich um eine Maschine oder Menschen handelt mit der er grade kommuniziert. Solche Maschinen sind, wenn sie nicht korrekt funktionieren bzw. wenn sie nicht den Erwartungen entsprechen auch eine große Gefahr. Es gibt ganze Forschungsbereiche (AI-Safety) die sich mit der sicheren Entwicklung von AGI beschaffen. Für die Entwicklung von kognitiven Maschinen gilt das maschinelle Lernen als eine Schlüsseltechnologie. Die Maschinen sollen dabei aus großen Datenmengen, die zum Trainieren genutzt werden selbst Muster erkennen und daraus Verhaltensregeln ableiten. Somit könnten Maschinen vielseitiger eingesetzt werden und ihre Leistung verbessern. Eine Voraussetzung für eine derartige Entwicklung sind die großen Datenmengen die mit dem nächsten Schlagwort „Big Data“ in Verbindung stehen, zusätzlich sind natürlich auch noch entsprechend schnelle Prozessoren nötig um effektiv zu lernen. Daher auch der jetzige Hype dieses Themas. Das Konzept des maschinellen Lernens ist schon lange bekannt aber früher haben die Datenmengen und die schnellen Prozessoren der heutigen Grafikkarten gefehlt, die es heute jedem möglichmachen sein eigenes Neuronales-Netz zu trainieren. Mit solchen Techniken können Maschinen die menschliche Sprache verarbeiten, Prozesse optimieren, in dem diese exakt analysiert werden, hochkomplexe Anlagen steuern und wie schon genannt in der Medizin Krankheitsbilder und Muster erkennen, die selbst Fachleuten nicht auffallen. 

Die Auswirkungen und Einsatzmöglichkeiten sind so groß und universell, dass selbst in der Science-Fiction diese sehr häufig nicht vollständig zu Ende gedacht wirken. Fürs erste werden wohl Anwendungsbereiche wie das autonome Fahren, automatischer Aktienhandel, Mustererkennung in der Medizin, Zustandsüberwachung von Industrieanlagen und Stromerzeugungsanlagen zu den Hauptaufgaben gehören. Dieses Thema und die ethischen Auswirkungen von KI werden in dem entsprechenden Kapitel nochmal genauer erläutert. [Quelle Buch]

## Saftey und Security

### Daten – der Treibstoff der modernen Welt

Wie in den vorherigen Kapiteln bereits beschrieben spielen die Daten in der modernen Welt eine riesige Rolle. Wie im Beispiel des 3D-Drucks bereits klar geworden ist reicht es aus im Besitz des 3D-Modell zu sein um das Objekt jederzeit beliebig oft und überall zu erstellen. Benötigt wird lediglich ein kompatibler 3D-Drucker, etwas Zeit und das Rohmaterial. Es gibt praktisch unendlich viele Beispiele in denen klar wird wie wichtig und wertvoll der Besitz von relevanten Daten und Informationen ist und das sich daraus in der Regel auch ein Wettbewerbsvorteil für Unternehmen ergibt. Neue Datenschutz Gesetze sollen Privatpersonen schützen aber wer zu sorglos und leichtsinnig mit seinen Daten umgeht ist häufig auch nicht mehr zu schützen. 

Die Komplexität von Systemen steigt und somit auch die Angriffsoberfläche, die Risiken und die Auswirkungen von Störungen. In den Ingenieurwissenschaften wird von „Resilienz“ gesprochen, wenn Systeme Ausfallsicherheit bieten und Teil-Ausfälle kompensieren oder ausgleichen können und im Wesentlichen ihre Funktionalität beibehalten. Diese Resilienz wird im Rahmen dieses Buches mit dem bisher beschrieben Saftey Aspekt gleichgesetzt. Resilienz, also Saftey sollte bei den heutigen Hightech-Systemen immer eine primäre Rolle spielen. [Quelle Buch]

### Industrial Data Space

Integrität von Daten und Informationen spielt eine immer wichtigere Rolle. Die Grundlage das Informationszeitalter stell der Austausch von Informationen dar. Dies ist wichtig, da diese wiederum Grundlage für Forschung und bedarfsgerechte Dienstleistungen sind. Beim Austausch oder Transport von Informationen bestehen immer mehrere Risiken. Die Daten können abgefangen, manipuliert, verfälscht oder blockiert werden. Je stärker Prozesse von Daten abhängig sind umso größer sind die Auswirkungen bei fehlerhaften oder verfälschten Daten. Die steigende Cyberkriminalität stellt hier ein großes Problem dar.

Von der Fraunhofer-Gesellschaft wurde, daher eine Initiative für den sogenannten „Industrial Data Space“ geschaffen. Diese Initiative soll einen sicheren Datenraum schaffen der branchenübergreifend eingesetzt werden kann um den Unternehmen verschiedenster Größe die Bewirtschaftung ihrer eigenen Daten und Kundendaten zu ermöglichen. Als Kernelemente für sichere Verarbeitung und Austausch von Daten dienen folgende Punkte: Schutz-, Governance-, Kooperations- und Kontrollmechanismen. Um die Umsetzung zu erleichtern wurde als Grundlage ein mehrschichtiges Referenzarchitekturmodell geschaffen, welches Anwendungsfälle abdeckt, in denen der Austausch von Daten essenziell ist. Unter anderen gehören dazu Anwendungen wie z.B. maschinelles Lernen, Produktionsoptimierung, Safety im Straßenverkehr, medizinische Diagnosen, Steuerung der Energieversorgung und die Entwicklung und Verbesserung von Dienstleistungen und Geschäftsmodellen. Für die erfolgreiche Einführung von erneuerbaren Energien im Rahmen der Energiewende wird die Digitalisierung ebenso benötigt. Der Grund ist, dass die Analyse und Steuerung von Verbrauch, Verfügbarkeit und Lasten ein Aufgabenberieche von digitalen Systemen ist. [Quelle Buch]

### Datenintegrität

Die hohe Verletzlichkeit von Daten stammt daher, dass diese wesentlich leichter und schneller manipuliert werden können als physische Objekte. Daten können einfach kopiert und vervielfältigt werden und in der Regel lassen sich mit kleinsten Änderungen im Quelltext einer Software gravierende Veränderungen provozieren. Neben der Verschlüsselung und Signierung von Dokumenten gehört auch die Sicherstellung von Einträgen und Transaktionen zu den Themen der Digitalisierung. Für letzteres wird die Tauglichkeit von Blockchain-Technologien überprüft. Für das aktuelle Hype-Thema Blockchain wurde in diesem Buch ein ganzes Kapitel reserviert, welches sehr ausführlich die Funktionsweise, Anwendungsfelder und unterschiedliche Blockchain-Technologien darstellt. 

### Cyber-Security

Wie bereits deutlich wurde ist eine digitale Gesellschaft abhängig von der geschaffenen Infrastruktur und somit steht der Security Aspekt ständig im Fokus. Es vergeht kaum eine Woche in den nicht von gestohlenen Daten in den IT-Nachrichten berichtet wird. Datenlecks und die ständigen Angriffe durch Unbefugte sorgen für ein regelrechtes Wettrüsten zwischen Softwareherstellern und Angreifern. Dabei geht es darum Daten, Dateninfrastrukturen, persönliche und personenbezogene Daten zu schützen. Die heutige Gesellschaft ist einer ständigen Gefährdung durch sich selbst ausgesetzt und Angriffe auf die Infrastruktur können verheerende Auswirkungen haben. Mögliche Angriffsziele sind Energie- und Wasserversorgung, Finanz- und Währungssysteme, Transportmittel, Industrie 4.0 Fertigungsanlagen und natürlich die Kommunikationssysteme wie Mobilfunknetze und das Internet selbst. Stellen Sie sich nur einmal vor, dass das Internet nur in Deutschland für eine Stunde ausfällt, alleine der wirtschaftliche Schade durch ausfallenden Börsenhandel wäre enorm. Daher muss man auf Angriffe entsprechend vorbereitet sein und diese frühzeitig erkennen und falls möglich abwehren oder zumindest den Schaden in Grenzen halten.

Obwohl es viele besser wissen oder wissen müssten verschlüsseln immer noch viele Menschen ihre E-Mails nicht. Ebenso wenig werden Passwort-Manager genutzt um die Sicherheit der herkömmlichen Benutzername-Passwort-Authentifizierung zu steigern. Viele Sicherheitssysteme werden oftmals einfach falsch verwendet oder fehlerhaft konfiguriert. Das liegt daran das bei den heutigen Systemen oft die Usability nicht gegeben ist. Die Einrichtung von im Grunde simpler Systeme die E-Mail Verschlüsselung und Passwort-Managern wird als schlichtweg als zu komplex und aufwendig gesehen.

Von Sicherheitsexperten wird daher der Mensch als unsicherstes Glied in sicherheitsrelevanten Systemen gesehen. Denn dieser ist anfällig für Social-Engineering-Angriffe und lässt sich von Angreifern belügen, betrügen, erpressen oder schlichtweg überreden, weil man ja mal eine Ausnahme machen kann. 

Wie oben beschrieben sollten die Menschen und die Usability im Mittelpunkt stehen und nicht wie es bisher war, dass die Menschen sich der Technik anpassen mussten. Daraus können auch ganze neue Konzepte für die Entwicklung und Forschung entstehen. Die Verschmelzung von biologischen und digitalen Konzepten wäre möglich, da wie schon gezeigt DAN und Maschinencode ähnlich und somit auch in gewisser Weise kompatible sind. Beispielsweise Schwarmintelligenz für die Logistik, lernende Roboter, Biosensorik, der schon angesprochene 3D-Druck und programmierbare Materialien zeigen in die beschriebene Richtung. [Quelle Buch]

## Mixed-Reality (Augmented-Reality und Virtual-Reality)

Technologien der virtuellen und erweiterten Realität, also „Augmented-Reality“ (AR) und „Virtual-Reality“ (VR), sind in vielen Anwendungsfelder zu finden. Aktuell sind beide Themen sehr aktuell und es existieren auch schon Produkte für Konsumenten. VR-Headsets von HTC, Oculus und Sony sind relativ weit verbreitet. AR wird auf dem Smartphone z.B. für Spiele eingesetzt und in der Industrie hat die „Google Glass“ eine zweite Chance bekommen nachdem diese bei Privatkunden aus Datenschutz Gründen nicht akzeptiert wurde. Die Entwicklung an der „Mircrosoft HoloLens“, einem Head-mounted-Display ist noch nicht abgeschlossen. In der Automobilindustrie werden „Head-up-Displays“ verbaut die eine Form der AG sind und Informationen über Geschwindigkeit usw. direkt auf die Windschutzscheibe projizieren.

Die beiden Begriff AR und VR sind Teil des Oberbegriffs „Mixed-Reality“ dieser umfasst das gesamte Realitäts-Virtualitäts-Kontinuum. Von der reinen echten Umgebung bis hin zur vollständigen VR. Dazwischen liegen AR bei, der die Realität mit digitalen Informationen erweitert wird und die „Augmented-Virtuality“ (AV), welche die Virtualität um Informationen aus der realen Umgebung erweitert. In der folgenden Abbildung ist das Kontinuum dargestellt. 

[https://de.wikipedia.org/wiki/Mixed_reality#/media/File:Mixed_Reality.png] [Wiki]

Die Wichtigkeit und die Vorteile der Digitalisierung von realen Objekten wie z.B. Kulturgütern wurde bereits erläutert. Dabei werden durch solche Initiativen Standards für den digitalen Zugang durch den Endverbraucher festgelegt. Projekte wie das „Google Books Library Project“ dessen Ziel es ist Millionen von Büchern zu digitalisieren treiben dies noch weiter voran. Auch die virtuelle Bibliothek „Europeana“ hat inzwischen über 30. Millionen Artefakte digitalisiert. Es fällt dabei auf derartige Anstrengungen primär auf 2D Objekte beschränkt sind, da die zur hochpräzisen Digitalisierung von dreidimensionalen Objekten die kommerziell verfügbare Technologie fehlt. Der hohe Zeitaufwand und die damit verbundenen Kosten sorgen dafür das aktuelle hauptsächlich nur prestigeträchtige Einzelstück wie die Büste der Nofretete digitalisiert werden. Der Zeitaufwand, der alleine für die Einstellung und korrekte Ausrichtung der Erfassungsgräte ist enorm und liegt in etwa bei 85% des gesamten Erfassungsprozesses. Zusätzlich wird die Digitalisierung von Objekten erschwert, da die Erfassung von einigen Materialien leider weiterhin eingeschränkt ist und in dem Bereich noch Forschungsbedarf besteht. [Quelle Buch]

### Anforderungen an Mixed-Reality

Die Umsetzung von VR und AR ist stark unterschiedlich. Diese reichen von Cloud-Infrastrukturen bis hin zum schon genannten Head-Mounted-Display, die in Verbindung mit unterschiedlichsten Rechenkapazitäten, Betriebssystemen und Ein- und Ausgabemöglichkeiten arbeiten. Diese Systeme sollen skalierbar sein und bei der hohen Plattform Diversität bringen grade die mobilen Systeme wieder den Security Aspekt in den Fokus. Vertrauliche Daten wie z.B. das CAD-Modelle des neuen Prototyps kabellos an das Ausgabegerät zu übertragen ohne dieses dort aus Sicherheitsgründen zu speichern stellt eine Herausforderung dar. Ähnliche Anforderungen lassen sich bei Web-Technologien finden. Daher werden VR/AR in Zukunft eng mit Web-Technologien verbunden sein um im puncto Sicherheit punkten zu können und ihre Einsetzbarkeit und Akzeptanz in der Industrie zu erhöhen. [Quelle Buch]

In der folgenden Abbildung werden Mixed-Reality Median wie z.B. Spiele und Filme mit den Anforderungen an Integrierbarkeit und Konfigurierbarkeit des Industriebereiches verglichen. 

[Bild: anforderungen_vr_ar.png] [Quelle Buch]

### Virtual-Reality 

In Europa wird, trotz der hohen Anforderungen, VR schon seit über 25 Jahren in der Industrie eingesetzt. Es werden 3D-Daten erlebbar gestaltet und in der Automobilindustrie und vielen weiteren Industriesträngen verdrängen digitale Mock-Ups (DMU) die realen und physikalischen Mock-Ups (PMU). Obwohl die Kosten für Software und Hardware von VR-Lösungen relativ hoch sind werden diese Investitionen betrieben. In den letzten fünf Jahren sind die Preise durch Verbesserungen von LCD und OLED Displays von vierstelligen in dreistellige Bereiche gewandert. Passende VR-Medien wie Filme und Spiele werden z.B. über die üblichen Plattformen wie „Steam“ vertrieben. In der Industrie wird jedoch die schon oben in der Abbildung dargestellt dynamische Konfiguration und Integrierbarkeit verlangt für die es keine durchgängigen Lösungen gibt. Die entwickelten Lösungen für Konsumentenprodukte lassen sich in der Regel nicht ohne weiteres auf die Industriellen Anwendungsfälle übertragen. 

Im Industriebereich haben Modelle eine viel höhere Qualität oder enthalten noch Informationen die später nicht mehr benötigt werden, zusätzlich sind die Daten häufig stark verteilt. Dies treibt die Datengrößen exponentiell nach oben was wiederum nach neuen Lösungen verlangt. Aktuell werden für Industrielösungen etablierte Dokumentenformate und Verfahren genutzt. 

Um jedoch bei der steigenden Datenmenge und Qualität der nötigen Bilder-pro-Sekunde („Frames-Per-Secound“ (FPS) einhalten zu können reicht jedoch häufig nicht einmal mehr die modernste Grafik-Hardware. Zu niedrige FPS kann zu Schwindelanfällen, Übelkeit und allgemeinem Unwohlsein bei der Bedienung von VR-Systemen führen. Um die FPS Zahlen hoch und stabil zu halten reicht es jedoch nicht mehr aus den Grafikspeicher und die Grafikprozessor Geschwindigkeit(Rasterisierung) nach oben zu schrauben. Grade im Bereich der Sichtbarkeits-Berechnung von Szenen müssen neue räumliche und hierarchische Datenstrukturen eingesetzt werden, die den Darstellungsprozess beschleunigen. Dann können althergebrachte Konzepte wie „Divide-and-Conquer“ eingesetzt werden um lediglich die Hüllkörper von Szenen-Elementen zu verarbeiten. Dafür werden erzeugte Baumstrukturen von Szenen-Elementen rekursiv zerlegt was nebenbei die Parallelisierbarkeit des Prozesses erhöht.

Um solche Datenstrukturen aufzubauen und zu erzeugen können ebenso bekannte Konzepte wie z.B. „Raytracing“ und „Collision-Detection“ genutzt werden. Die resultierenden Datenstrukturen sind z.B. K-D-Bäume, Bounding-Volume-Hierarchie, Bounding-Interval-Hierarchy, reguläre Gitter und Octrees. Je nach Forschungsfeld und Anwendungsfeld werden diese Ansätze stetig verbessert und optimiert.

Da in der Industrie die Datensätze so groß sind reicht der Hauptspeicher (RAM) der Computer häufig nicht aus und sogenannte „Out-of-Core-Technologien“ sollen dabei helfen Daten On-Demand und On-the-fly aus dem Sekundärspeicher (SSD/HDD) zu laden. Bei Konsumentenprodukten, die auf einem herkömmlichen Betriebssystem laufen ist dies in der Regel nicht möglich, da für die Umsetzung von Out-of-Core-Technologien vollständiger Zugriff auf das Speichermanagement sowie die Caching Verfahren des Systems vorhanden sein muss. Bei dieser Technologie steht in erster Line die Performanz hinsichtlich von harten Echtzeitanforderungen im Vordergrund und nicht die exakte bzw. vollständige und korrekte Visualisierung.

Um tatsächlich die Echtzeitanforderungen zu erfüllen muss die räumliche und zeitliche Kohärenz ausgenutzt werden. Die hierarchischen Datenstrukturen, die oben beschrieben wurden liefern die räumliche Kohärenz. Die zeitliche Kohärenz also der frame-übergreifende zeitliche Zusammenhang muss durch den gewählten Render-Algorithmus gelöst werden. Zusätzlich muss eine Verdeckungsberechnungen („Occlusion-Culling“) von mehreren überlappenden Szenen-Elementen durch geführt werden. Dazu kann z.B. der „Coherent-Hierarchical-Culling“ eingesetzt werden.

### Augmented-Reality

In der Industrie 4.0 sollen in Zukunft die Fertigungsqualität mit dem sogenannten „SOLL-IST-Abgleich“ gewährleistet werden. Dabei wird die so beschrieben „Cyber-physikalische Äquivalenz durch den ständigen Abgleich von Simulationsprozessen und Fertigungsprozessen sichergestellt. Hierbei wird wieder der schon beschriebene digitale Zwilling genutzt, um IST-Daten zurück in die agile Produktionsplanung zuführen.

Genau dieser Anwendungsfall kann mit AR bedient werden und somit werden solche Verfahren hier relevant. Es gilt also die SOLL-IST-Differenz in Echtzeit zu ermitteln und bei Abweichungen können entsprechende Einblendungen direkt in dem Umfeld visualisiert werden. In diesen Bereichen hat sich AR auch schon fest positioniert und wird für Plannungs- und Prüfprozesse verwendet. Dazu gehören z.B. folgende:

* AR-Reparaturanleitung: KFZ-Mechaniker können mit AR-Systemen bei hochkomplexen Reparaturszenarien unterstützt werden. Dafür werden die eingeblendeten Schritte auf das Fahrzeug angepasst und es kann somit auch die Konfiguration und Ausstattung berücksichtig werden. 
* AR-gestützte-Wartung: Ähnlich wie im vorherigen Punkt wird bei der AR-gestützten-Wartung ein Experte mit einbezogen. Über die AR-Systeme kann in der Regel ein Livestream gestartet werden, der dann via Internet an einen Fachmann übertragen werden kann. Dieser kann dann Tipps und weitere Hilfestellung geben die der herkömmlichen AR-Reparaturanleitung hinzugefügt werden und beim nächsten Mal sind diese Informationen Teil der Reparaturanleitung.
* AR-Handbücher: Da eine AR-System graphisch ganz andere Möglichkeiten bietet als ein herkömmliches Buch können so auch graphische Anleitung und Handbücher zu komplexen Systemen erstellt werden. Eine größtenteils graphische Benutzeranleitung ist praktisch sprachunabhängig und über den App-Store der entsprechenden Plattform können diese leichter aktualisiert werden.

AR-Technologie ist eine wichtige Entwicklung, da durch die neuen Hardwaresysteme neue Interaktionsparadigmen ermöglich werden. Somit werden Prozesse wie Produktionsplanung und Qualitätskontrolle komplett revolutioniert. Wie oben schon genannt ist die Microsoft HoloLens eines dieser Systeme. Die HoloLens bietet multimodale Sensorik, welche direkt in das Gerät integriert ist. Zusätzlich bietet das Systemeine hohe Bildqualität. An der HoloLens lassen sich aber auch gut die Einschränkungen im Profibereich wie folgt erläutern:

* Aufwendige Content-Aufbereitung: 3D-Modelle müssen in der Regel händisch bearbeitet werden damit sie die Anzahl von empfohlenen Polygonen nicht überschreiten. Empfohlen werden 100.000 und ein übliches PKW-CAD-Modell besteht aus ca. 80 Millionen. Es müssen also zusätzlich qualitativ schlechtere Modelle für die Anzeige genutzt werden, die ggf. erst erstellt werden müssen.
* Tracking mit der HoloLens: Die HoloLens verwendet das sogenannte „Simultanous Localisation and Mapping“ (SLAM). Damit ist es möglich Fenster und Apps in der Umgebung anzupinnen. Dies geschieht über eine spezielle Geste des Benutzers. Allerdings ist dies z.B. für den gewünschten SOLL-IST-Abgleich nicht ausreichend. 
* Tracking in statischen Umgebungen: Während der Laufzeit baut die Anwendung für das Tracking 3D-Rekonstruktionen aus 3D-Featuremaps und 3D-Meshes. Daher nimmt die Qualität des Trackings in dynamischen Umgebungen deutlich ab. Dies tritt auf, wenn mehrere HoloLens-Systeme von unterschiedlichen Benutzern verwendet werden oder wenn die zu trackenden Komponenten bewegt werden. 
* Tracking von CAD-Modellen: Das Tracking mit SLAM kann nicht zwischen unterschiedlichen Objekten, die getrackt werden sollen unterscheiden. Somit ist es für SLAM auch nicht möglich zwischen einem Objekt und dem Hintergrund zu differenzieren. Für den gewünschten Anwendungsfall des SOLL-IST-Abgleich ist das System also weiterhin nicht geeignet, wenn dafür die Ausrichtung eines Objekts in Relation zur Referenzgeometrie geprüft werden muss.
* Gestenbasierte Interaktion: Die Gestensteuerung wurde nicht primär für industrielle Anwendungen entwickelt und ist daher nicht immer passend geeignet. In Verbindung mit einem Tablet kann dies behoben werden, dieses wird genutzt um die Interaktion zu steuern und die HoloLens diese Visualisiert.
* Security: Aktuell müssen die reduzierten 3D-Modelle direkt auf der HoloLens gespeichert oder zumindest zwischen gespeichert werden. Da das System W-Lan fähig ist könnte dies für Datensicherheit und Datenkonsistenz inakzeptable sein.

Wenn das System auf eine Client-Server-Infrastruktur umgestellt werden würde könnten viele der Einschränkungen umgangen werden. Es sollte in jedem Fall sichergestellt werden das 3D-Modelle das Produktdatenmanagement-System (PDM) nicht verlassen, da diese in der Regel von hohem Wert sind und die „intellectual property“ (IP) des Unternehmens darstellt. An den Client, also an die HoloLens sollten dann mit Videostreaming-Technologie nur die relevanten Inhalte als Einzelbilder geschickt werden. Als eine alternative könnten sogenannte „deferred shading“ und „G-Buffer“ verschickt werden die für die jeweils aktuellen Views vorberechnet werden. Wie schon angemerkt können für die Umsetzung solcher Systeme die Web-Technologien und die damit verbundenen Dienstarchitekturen verwendet werden. Aktuell können solche Systeme sogar schon mit Bibliotheken wie „WebGl/WebCL“ umgesetzt werden. Diese können ohne Plugins und performant mit einer On-Chip-Verarbeitung im Web-Browser implementiert werden. Für VR/AR im Industriebereich bietet Web-Technologie folgende Vorteile:

* Security: Wenn der Webbrowser der verschiedensten Endgeräte (Tablet, PC, Thin Client) genutzt wird müssen auch keine nativen Softwarekomponenten von dritten genutzt werden die potentiell unsicher sein könnten.
* Plattformunabhängigkeit: Web-Technologien können größtenteils plattformunabhängig implementiert werden und sind so nicht von dem Betriebssystem wie z.B. iOS, Android und Windows 10 abhängig. Dadurch werden auch die Entwicklungskosten gesenkt, da keine Parallelentwicklung nötig ist.
* Skalier- und Verteilbarkeit: Wenn der größte Teil oder sogar alle Berechnungen von Modellen auf dem Server betrieben werden können die Endgeräte schlanker ausfallen. Mit einer Client-Server-Infrastruktur kann besser auf erhöhte Last reagiert werden indem dynamisch zusätzliche Server dazu geschaltet werden.

Ein weiterer wichtiger Vorteil ist, dass bei einer solchen Infrastruktur immer auf die aktuellsten Daten zugegriffen werden kann. Dies ist besonders wichtig, wenn der schon so häufig angesprochene SOLL-IST-Abgleich durchgeführt werden soll. Für Planungs- und Entwicklungsprozesse ist es ebenso wichtig auf den aktuellsten Daten zu arbeiten. [Quelle Buch]



<!-- COMMENT!

Ein

<img src="./images/merkle_tree.png" width="400">

Aufbau eines einzelnen Blocks.
Abbildung entnommen und angepasst aus
<a>[[VUJI18]](#ref_VUJI18)</a>


# Literaturverzeichnis
<a name="ref_abey16">[ABEY16]</a>:Abeyratne, Saveen ; Monfared, Radmehr: Blockchain Ready Manufacturing Supply Chain Using Distributed Ledger. International Journal of Research in Engineering and Technology. 05. 2016

<a name="ref_beck16">[BECK16]</a>:Beck, Roman ; Stenum Czepluch, Jacob; Lollike, Nikolaj; and Malone, Simon: BLOCKCHAIN – THE GATEWAY TO TRUST-FREE CRYPTOGRAPHIC TRANSACTIONS. Twenty-Fourth European Conference on Information Systems (ECIS), İstanbul,Turkey, 2016. Springer Publishing Company, 2016. p. 1-14.

<a name="ref_Beck17">[BECK17]</a>: Beckel, Christoph: Skalieren Blockchains? Sorgen und Lösungsansätze, 2017, URL: http://site.blocklab.de/2017/Skalierung/ (letzter Zugriff: 12.05.2018)

<a name="ref_bigg18">[BIGG18]</a>: Biggs, John (Techcrunch) : Exit scammers run off with $660 million in ICO earnings. Web-Quelle, 2018,
[https://techcrunch.com/2018/04/13/exit-scammers-run-off-with-660-million-in-ico-earnings/](https://techcrunch.com/2018/04/13/exit-scammers-run-off-with-660-million-in-ico-earnings/) (letzter Zugriff: 13.05.2018)

-->