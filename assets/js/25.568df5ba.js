(window.webpackJsonp=window.webpackJsonp||[]).push([[25],{149:function(e,n,i){e.exports=i.p+"assets/img/perc-ff-dff.b3d7829a.png"},150:function(e,n,i){e.exports=i.p+"assets/img/multi-perceptron.93481006.png"},151:function(e,n,i){e.exports=i.p+"assets/img/separable.89ae6956.png"},152:function(e,n,i){e.exports=i.p+"assets/img/activation-functions.db11840b.png"},153:function(e,n,i){e.exports=i.p+"assets/img/perceptron-form.5cfb3791.png"},154:function(e,n,i){e.exports=i.p+"assets/img/perceptron2.23d98080.png"},155:function(e,n,i){e.exports=i.p+"assets/img/perceptron.5c561670.png"},167:function(e,n,i){"use strict";i.r(n);var r=[function(){var e=this,n=e.$createElement,r=e._self._c||n;return r("div",{staticClass:"content"},[r("h1",{attrs:{id:"neural-networks"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#neural-networks","aria-hidden":"true"}},[e._v("#")]),e._v(" Neural networks")]),r("p",[e._v("Neuronale Netze (neural networks) sind in der Informatik, Netzwerke aus künstlichen Neuronen. Sie sind als Nachbildung der Neuronenstruktur von menschlichen Gehirnen entwickelt worden.")]),r("p",[e._v("Grundbaustein aller neuronalen Netzwerke ist das Perzeptron.")]),r("h2",{attrs:{id:"perzeptron"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#perzeptron","aria-hidden":"true"}},[e._v("#")]),e._v(" Perzeptron")]),r("p",[e._v("In seiner einfachsten Ausführung ist ein Perzeptron vergleichbar mit einem Logikschalter. Es kann boolesche Werte (1 - wahr, 0 - falsch) als Eingabe aufnehmen und gibt selbst wieder einen booleschen Wert als Ergebnis aus. In der Grafik kann das Perzeptron die logische Und-Verknüpfung zweier Werte ausführen. X und Y sind jeweils Eingaben, das Perzeptron gibt als Ausgabe entweder 1 aus, wenn beide Eingaben 1 sind, andernfalls 0.\n"),r("img",{staticStyle:{margin:"20px"},attrs:{src:i(155),width:"300"}})]),r("p",[e._v("Genauer betrachtet, verfügt ein Perzeptron über Eingabegewichtungen (w), eine Eingabe Funktion (input function), sowie eine Aktivierungsfunktion g() (activation function).\nDie Eingabegewichtungen sorgen dafür, dass die einzelnen Eingabewerte unterschiedlich stark in die Ausgabe einfließen können. Dazu wird über die Eingabefunktion eine Summe über alle Eingabewerte mit ihren jeweiligen Gewichtungsfaktoren (w) gebildet. Der errechnete Wert wird der Aktivierungsfunktion übergeben, die beeinflusst, ob das Perzeptron eine Aktivierung weiterleitet.")]),r("img",{staticStyle:{margin:"20px"},attrs:{src:i(154),width:"500"}}),e._v("\nQuelle: "),r("a",[e._v("[[RUSS16]](#ref_russ16)")]),r("p",[e._v("Die Ausgabe der "),r("em",[e._v("input function")]),e._v(" an die "),r("em",[e._v("activation function")]),e._v(" lässt sich folgendermaßen berechnen:")]),r("img",{staticStyle:{margin:"20px"},attrs:{src:i(153),width:"500"}}),r("p",[e._v("Wie in der Formel ersichtlich, erfolgt eine Weiterleitung von 1 an die "),r("em",[e._v("activation function")]),e._v(" wenn die Summe über die Produkte, zwischen Eingabevektorwert mit dem jeweiligen Gewicht (w), größer bzw. gleich dem Bias Gewicht (w 0,i) ist, andernfalls wird 0 weitergegeben.")]),r("h2",{attrs:{id:"aktivierungsfunktionen"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#aktivierungsfunktionen","aria-hidden":"true"}},[e._v("#")]),e._v(" Aktivierungsfunktionen")]),r("p",[e._v("Zuletzt entscheidet die Aktivierungsfunktion über eine Aktivierung des Perzeptrons. Verschiedene Altivierungsfunktionen eignen sich für unterschiedliche Problemstellungen. In der Grafik ist auf der linken Seite ist die Sprungfunktion und auf der rechten Seite die sigmoide Funktion zu sehen. Die Sprungfunktion leitet eine Aktivierung des Perzeptrons ein sobald die Ausgabe der "),r("em",[e._v("input function")]),e._v(" größer 0 ist. Eine sigmoide Funktion würde in einem anderem Beispiel feinere Abstufungen erlauben, wenn die Ausgabe einer "),r("em",[e._v("input function")]),e._v(" nicht nur 0 oder 1 annehmen können und wird in neueren neuronalen Netzen überwiegend verwendet.")]),r("img",{staticStyle:{margin:"20px"},attrs:{src:i(152),width:"400"}}),r("h2",{attrs:{id:"limitierung"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#limitierung","aria-hidden":"true"}},[e._v("#")]),e._v(" Limitierung")]),r("p",[e._v("Ein einzelnes Perzeptron ist fähig die logischen Funktionen AND, OR und NOT abzubilden. Minsky und Papert wiesen 1969 nach, dass die XOR Funktion nicht abgebildet werden kann. Auch kann es nur linear separable Problemstellungen lösen, für komplexere Klassifizierungen ist ein mehrlagiges Perzeptron nötig, dieses kann erstmals als neuronales Netz bezeichnet werden.")]),r("img",{staticStyle:{margin:"20px"},attrs:{src:i(151),width:"400"}}),r("h2",{attrs:{id:"mehrlagiges-perzeptron"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#mehrlagiges-perzeptron","aria-hidden":"true"}},[e._v("#")]),e._v(" Mehrlagiges Perzeptron")]),r("img",{staticStyle:{margin:"20px"},attrs:{src:i(150),width:"400"}}),r("p",[e._v("Mit mehrlagigen Perzeptren sind komplexere Funktionen berechenbar, in oben stehendem Beispiel die logische XOR Funktion welche mit einem 2-lagigem Perzeptron berechnet werden kann. Es kann hierbei auch von "),r("em",[e._v("feed-forward neural network")]),e._v(" gesprochen werden, "),r("em",[e._v("feed-forward")]),e._v(" deshalb, weil Aktivierungen nur Richtung Ausgabe weitergereicht werden und diese keine Veränderungen in vorherigen Schichten ("),r("em",[e._v("layer")]),e._v(") des neuronalen Netzes bewirken. Bei neuralen Netzwerken, die mehr als eine verdeckte Schicht ("),r("em",[e._v("hidden layer")]),e._v(") zwischen Eingabe- und Ausgabe-Neuron besitzen, wird die Bezeichnung "),r("em",[e._v("deep neural network")]),e._v(" bzw. "),r("em",[e._v("deep feed-forward network")]),e._v(" verwendet.\n"),r("img",{staticStyle:{margin:"10px"},attrs:{src:i(149),width:"600"}}),e._v("\nQuelle: Fjodor van Veen, Asimov Institute")]),r("h2",{attrs:{id:"literaturverzeichnis"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#literaturverzeichnis","aria-hidden":"true"}},[e._v("#")]),e._v(" Literaturverzeichnis")]),r("p",[r("a",{attrs:{name:"ref_russ16"}},[e._v("[RUSS16]")]),e._v(":Russell, Stuart J., and Peter Norvig. Artificial intelligence: a modern approach. Malaysia; Pearson Education Limited,, 2016.")])])}],t=i(0),s=Object(t.a)({},function(){this.$createElement;this._self._c;return this._m(0)},r,!1,null,null,null);n.default=s.exports}}]);